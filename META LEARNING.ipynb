{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUNNYA-P-GAUR/Machine-Learning/blob/main/META%20LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m9k_GBmmDQ3H"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple task\n",
        "def generate_task():\n",
        "    x = np.random.rand(10)\n",
        "    y = 2 * x + 1\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "h6yD0xKpF_rp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a model with random parameters\n",
        "def initialize_model():\n",
        "    return {'w': np.random.rand(), 'b': np.random.rand()}\n"
      ],
      "metadata": {
        "id": "2SRSvQ6kGLdw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "def loss(model, x, y):\n",
        "    y_pred = model['w'] * x + model['b']\n",
        "    return np.mean((y_pred - y)**2)"
      ],
      "metadata": {
        "id": "3oblQ58lGOVw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize meta-parameters\n",
        "meta_learning_rate = 0.1\n",
        "num_meta_iterations = 100"
      ],
      "metadata": {
        "id": "8nQRYL3XGRsQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta-training loop\n",
        "for meta_iteration in range(num_meta_iterations):\n",
        "    task = generate_task()\n",
        "    model = initialize_model()\n",
        "\n",
        "    # Perform task-specific gradient descent\n",
        "    task_learning_rate = 0.01\n",
        "    num_task_iterations = 10\n",
        "\n",
        "    for _ in range(num_task_iterations):\n",
        "        x, y = task\n",
        "        gradients = {}\n",
        "        for param in model:\n",
        "            original_param_value = model[param]\n",
        "            # Calculate the gradient for each parameter\n",
        "            model[param] = original_param_value + task_learning_rate\n",
        "            loss_plus = loss(model, x, y)\n",
        "            model[param] = original_param_value - task_learning_rate\n",
        "            loss_minus = loss(model, x, y)\n",
        "            model[param] = original_param_value\n",
        "            gradients[param] = (loss_plus - loss_minus) / (2 * task_learning_rate)\n",
        "\n",
        "        # Update the model with gradients\n",
        "        for param in model:\n",
        "            model[param] -= meta_learning_rate * gradients[param]\n",
        "\n",
        "    print(f\"Meta-Iteration {meta_iteration+1}: Loss = {loss(model, x, y)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUvUCrHsGUVp",
        "outputId": "ba0f97e2-3c62-4131-c4c9-b664a2e426e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-Iteration 1: Loss = 0.10797297637134018\n",
            "Meta-Iteration 2: Loss = 0.0716137399447107\n",
            "Meta-Iteration 3: Loss = 0.06843525144822929\n",
            "Meta-Iteration 4: Loss = 0.023626660445431966\n",
            "Meta-Iteration 5: Loss = 0.01572868029687378\n",
            "Meta-Iteration 6: Loss = 0.03552892172412518\n",
            "Meta-Iteration 7: Loss = 0.017255035226660546\n",
            "Meta-Iteration 8: Loss = 0.07120208198977375\n",
            "Meta-Iteration 9: Loss = 0.04680298641845142\n",
            "Meta-Iteration 10: Loss = 0.055220974117517196\n",
            "Meta-Iteration 11: Loss = 0.023459290517156124\n",
            "Meta-Iteration 12: Loss = 0.015579433062075976\n",
            "Meta-Iteration 13: Loss = 0.06526381574933535\n",
            "Meta-Iteration 14: Loss = 0.026254772106232043\n",
            "Meta-Iteration 15: Loss = 0.07453082859331175\n",
            "Meta-Iteration 16: Loss = 0.08376788903939549\n",
            "Meta-Iteration 17: Loss = 0.08512007714310829\n",
            "Meta-Iteration 18: Loss = 0.07295724089846944\n",
            "Meta-Iteration 19: Loss = 0.11659310728536246\n",
            "Meta-Iteration 20: Loss = 0.05104268083174762\n",
            "Meta-Iteration 21: Loss = 0.052850498238886955\n",
            "Meta-Iteration 22: Loss = 0.08242098718011667\n",
            "Meta-Iteration 23: Loss = 0.022239143894767523\n",
            "Meta-Iteration 24: Loss = 0.03179045243504518\n",
            "Meta-Iteration 25: Loss = 0.048751537448355496\n",
            "Meta-Iteration 26: Loss = 0.0646101512186952\n",
            "Meta-Iteration 27: Loss = 0.06867951205226898\n",
            "Meta-Iteration 28: Loss = 0.03377296737720668\n",
            "Meta-Iteration 29: Loss = 0.09372923933975422\n",
            "Meta-Iteration 30: Loss = 0.08940735292189177\n",
            "Meta-Iteration 31: Loss = 0.03950094590267757\n",
            "Meta-Iteration 32: Loss = 0.09266090058002244\n",
            "Meta-Iteration 33: Loss = 0.035367434111558675\n",
            "Meta-Iteration 34: Loss = 0.03262829800493712\n",
            "Meta-Iteration 35: Loss = 0.02218158160866758\n",
            "Meta-Iteration 36: Loss = 0.07988864551491208\n",
            "Meta-Iteration 37: Loss = 0.07765043884417591\n",
            "Meta-Iteration 38: Loss = 0.03253062186260859\n",
            "Meta-Iteration 39: Loss = 0.06345476750822415\n",
            "Meta-Iteration 40: Loss = 0.028731185807469316\n",
            "Meta-Iteration 41: Loss = 0.13169926148791541\n",
            "Meta-Iteration 42: Loss = 0.027568860161776638\n",
            "Meta-Iteration 43: Loss = 0.039869726734029845\n",
            "Meta-Iteration 44: Loss = 0.04066460907039448\n",
            "Meta-Iteration 45: Loss = 0.12030031025402928\n",
            "Meta-Iteration 46: Loss = 0.10040453553131892\n",
            "Meta-Iteration 47: Loss = 0.067983408256132\n",
            "Meta-Iteration 48: Loss = 0.03698305964471769\n",
            "Meta-Iteration 49: Loss = 0.07437089263258755\n",
            "Meta-Iteration 50: Loss = 0.04639634499277012\n",
            "Meta-Iteration 51: Loss = 0.06240903611976818\n",
            "Meta-Iteration 52: Loss = 0.052548788391037535\n",
            "Meta-Iteration 53: Loss = 0.11931632721191572\n",
            "Meta-Iteration 54: Loss = 0.05369175392853228\n",
            "Meta-Iteration 55: Loss = 0.07599460137988556\n",
            "Meta-Iteration 56: Loss = 0.04936924019769855\n",
            "Meta-Iteration 57: Loss = 0.1830821363219278\n",
            "Meta-Iteration 58: Loss = 0.047055057344253484\n",
            "Meta-Iteration 59: Loss = 0.13764206340988058\n",
            "Meta-Iteration 60: Loss = 0.021444805856723807\n",
            "Meta-Iteration 61: Loss = 0.04890758088230964\n",
            "Meta-Iteration 62: Loss = 0.09096578172565555\n",
            "Meta-Iteration 63: Loss = 0.019949260852681762\n",
            "Meta-Iteration 64: Loss = 0.12505523099143173\n",
            "Meta-Iteration 65: Loss = 0.02537822385213821\n",
            "Meta-Iteration 66: Loss = 0.07088696563371297\n",
            "Meta-Iteration 67: Loss = 0.049442809396359755\n",
            "Meta-Iteration 68: Loss = 0.10850572270961276\n",
            "Meta-Iteration 69: Loss = 0.14061893999541736\n",
            "Meta-Iteration 70: Loss = 0.04203809022018243\n",
            "Meta-Iteration 71: Loss = 0.015908800921706198\n",
            "Meta-Iteration 72: Loss = 0.15704620139189107\n",
            "Meta-Iteration 73: Loss = 0.07371166241279041\n",
            "Meta-Iteration 74: Loss = 0.11062541979148158\n",
            "Meta-Iteration 75: Loss = 0.09379783081863421\n",
            "Meta-Iteration 76: Loss = 0.15016321692532414\n",
            "Meta-Iteration 77: Loss = 0.06862179039479668\n",
            "Meta-Iteration 78: Loss = 0.05648220841984697\n",
            "Meta-Iteration 79: Loss = 0.044641604735865564\n",
            "Meta-Iteration 80: Loss = 0.05126301640913692\n",
            "Meta-Iteration 81: Loss = 0.03052137970746418\n",
            "Meta-Iteration 82: Loss = 0.145850424024636\n",
            "Meta-Iteration 83: Loss = 0.03145511926088494\n",
            "Meta-Iteration 84: Loss = 0.056179003362602245\n",
            "Meta-Iteration 85: Loss = 0.02130516900389326\n",
            "Meta-Iteration 86: Loss = 0.09796485183890705\n",
            "Meta-Iteration 87: Loss = 0.049466123563791674\n",
            "Meta-Iteration 88: Loss = 0.09791626165371364\n",
            "Meta-Iteration 89: Loss = 0.02329376495592183\n",
            "Meta-Iteration 90: Loss = 0.058429166969459656\n",
            "Meta-Iteration 91: Loss = 0.011178655635349404\n",
            "Meta-Iteration 92: Loss = 0.061868833771179355\n",
            "Meta-Iteration 93: Loss = 0.08178286329658604\n",
            "Meta-Iteration 94: Loss = 0.06287958065106056\n",
            "Meta-Iteration 95: Loss = 0.04480272688008654\n",
            "Meta-Iteration 96: Loss = 0.04826995118801154\n",
            "Meta-Iteration 97: Loss = 0.02337350459346138\n",
            "Meta-Iteration 98: Loss = 0.056906451689421746\n",
            "Meta-Iteration 99: Loss = 0.04661214510437109\n",
            "Meta-Iteration 100: Loss = 0.03927121961236076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlw8svKQGrc5"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}